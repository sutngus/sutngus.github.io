---
layout: post
title:  "5. 트리 알고리즘"
date : 2024-08-17 00:25:20 +0700
---

# 5-1 결정트리      
입고된 와인의 표시(화이트/ 레드)가 누락되었다. 알코올 도수 당도, pH 값으로 와인의 종류를 구별해보자      
먼저 **로지스틱 회귀모델**을 적용해보자.      
![image](https://github.com/user-attachments/assets/9fd637a9-3780-4416-9691-507ddc230a38)      
- 판다스 데이터프레임의 유용한 메서드: **info()**: 각 열의 데이터 타입과 누락된 데이터 확인/ **describe()**: 열에 대한 간략한 통계      
![image](https://github.com/user-attachments/assets/2edceb19-d4cc-4d93-85ec-e4c6e422cab4)      
- 1) 데이터프레임을 넘파이배열로 바꾸고, 2) 훈련세트와 테스트세트로 나누고, 3) 사이킷런의 StandardScaler 클래스를 이용해 특성들을 표준화해 스케일을 맞추어준다.      
![image](https://github.com/user-attachments/assets/c4b0e997-3b32-4da6-9732-938c4c6b7836)      
- 4) 로지스틱 회귀 모델 훈련 - 과소적합       
![image](https://github.com/user-attachments/assets/9b4f1b43-444d-48a2-9797-21851c07bdb5)      
![image](https://github.com/user-attachments/assets/92f0a89f-19dc-48eb-b39d-68b26b96ea05)      
우리는 모델이 왜 저런 계수 값을 학습했는지 정확히 이해하기 어려움 ---> 쉽게 설명할 수 있는 모델 = **결정트리**      

## 결정트리      
**결정트리**: 예/ 아니오 에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘. 예측과정을 이해하기 쉽고 성능도 뛰어남      
```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
![image](https://github.com/user-attachments/assets/3c881417-7e01-4b86-9a3d-a02cbe5d1713)      
너무 복잡하니 트리의 깊이를 제한해서 출력해보자.       
- max_depth: 트리가 성장할 최대 깊이, filled: 클래스에 따른 노드의 색(어떤 클래스의 비율이 높아지면 점점 진한 색이 됨), feature_name: 특성의 이름 전달      
```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![image](https://github.com/user-attachments/assets/336c8409-58d9-4c5e-b602-5205e64e912a)
![image](https://github.com/user-attachments/assets/ae51756f-40d9-4b7e-a182-d77c4fcd3a91)       
- value에서 왼쪽이 음성클래스, 오른쪽이 양성클래스      
- 리프노드에서 가장 많은 클래스가 예측 클래스가 됨      

### 불순도      
**불순도**: 얼마나 다양한 클래스가 섞여 있는지, 결정트리가 최적의 질문을 찾기 위한 기준, 데이터를 분할할 기준 - 지니불순도, 엔트로피불순도      
- DecisionTreeClassifier 클래스의 criterion 매개변수의 기본 값이 'gini'임      
- 지니불순도 = 1- (음성클래스의 비율^2 + 양성클래스의 비율^2)      
- 두 개의 클래스가 완전히 섞여 있다면 불순도가 높음. 두 클래스의 비율이 정확히 1/2씩 이라면 지니불순도가 0.5가 되어 최악임(최댓값). 순수노드는 0임      
- 부모노드와 자식노드의 불순도 차이(**정보이득**)가 가능한 크도록, 즉 자식노드의 불순도를 최대한 낮아지게 하도록 트리를 성장시킴     
- **정보이득**(불순도 차이) = 부모의 불순도 - (왼쪽노드 샘플 수/부모의 샘플 수)x왼쪽노드 불순도 - (오른쪽노드 샘플 수/부모의 샘플 수)x오른쪽노드 불순도       
- 엔트로피 불순도 = -음성클래스의 비율xlog2(음성클래스의 비율) - 양성클래스의 비율xlog2(양성클래스의 비율)      
- 지니 불순도와 엔트로피 불순도가 만든 결과의 차이는 크지 않음      
- 이렇듯 불순도를 기준으로 샘플을 나누고, 불순도는 클래스별 비율을 가지고 계산한다. 따라서 특성값의 스케일은 계산에 영향을 미치지 않기 때문에, **결정트리는 표준화 전처리 과정이 필요없다**. -> 전처리 전의 훈련세트와 테스트세트로 모델을 훈련시키면 score점수가 동일하게 나온다.      

### 가지치기      
- 결정트리는 제한 없이 성장하면 훈련세트에 과대적합되기 쉽기 때문에 **가지치기**로 결정트리의 성장을 제한해주어야함      
- 가장 간단한 방법은 최대 깊이(max_depth)를 정하는 것      
```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
```
- **특성중요도**: 결정트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값      
- 각 노드의 정보이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더하여 계산      
- 이를 활용하여 결정트리모델을 특성 선택에 활용할 수 있음      
```python
print(dt.feature_importances_)
```
[0.12345626 0.86862934 0.0079144 ]      
- 
