---
layout: post
title:  "5. 트리 알고리즘"
date : 2024-08-17 00:25:20 +0700
---

# 5-1 결정트리      
입고된 와인의 표시(화이트/ 레드)가 누락되었다. 알코올 도수 당도, pH 값으로 와인의 종류를 구별해보자      
먼저 **로지스틱 회귀모델**을 적용해보자.      
![image](https://github.com/user-attachments/assets/9fd637a9-3780-4416-9691-507ddc230a38)      
- 판다스 데이터프레임의 유용한 메서드: **info()**: 각 열의 데이터 타입과 누락된 데이터 확인/ **describe()**: 열에 대한 간략한 통계      
![image](https://github.com/user-attachments/assets/2edceb19-d4cc-4d93-85ec-e4c6e422cab4)      
- 1) 데이터프레임을 넘파이배열로 바꾸고, 2) 훈련세트와 테스트세트로 나누고, 3) 사이킷런의 StandardScaler 클래스를 이용해 특성들을 표준화해 스케일을 맞추어준다.      
![image](https://github.com/user-attachments/assets/c4b0e997-3b32-4da6-9732-938c4c6b7836)      
- 4) 로지스틱 회귀 모델 훈련 - 과소적합       
![image](https://github.com/user-attachments/assets/9b4f1b43-444d-48a2-9797-21851c07bdb5)      
![image](https://github.com/user-attachments/assets/92f0a89f-19dc-48eb-b39d-68b26b96ea05)      
우리는 모델이 왜 저런 계수 값을 학습했는지 정확히 이해하기 어려움 ---> 쉽게 설명할 수 있는 모델 = **결정트리**      

## 결정트리      
**결정트리**: 예/ 아니오 에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘. 예측과정을 이해하기 쉽고 성능도 뛰어남      
```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
![image](https://github.com/user-attachments/assets/3c881417-7e01-4b86-9a3d-a02cbe5d1713)      
너무 복잡하니 트리의 깊이를 제한해서 출력해보자.       
- max_depth: 트리가 성장할 최대 깊이, filled: 클래스에 따른 노드의 색(어떤 클래스의 비율이 높아지면 점점 진한 색이 됨), feature_name: 특성의 이름 전달      
```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![image](https://github.com/user-attachments/assets/336c8409-58d9-4c5e-b602-5205e64e912a)
![image](https://github.com/user-attachments/assets/ae51756f-40d9-4b7e-a182-d77c4fcd3a91)       
- value에서 왼쪽이 음성클래스, 오른쪽이 양성클래스      
- 리프노드에서 가장 많은 클래스가 예측 클래스가 됨      

### 불순도      
**불순도**: 얼마나 다양한 클래스가 섞여 있는지, 결정트리가 최적의 질문을 찾기 위한 기준, 데이터를 분할할 기준 - 지니불순도, 엔트로피불순도      
- DecisionTreeClassifier 클래스의 criterion 매개변수의 기본 값이 'gini'임      
- 지니불순도 = 1- (음성클래스의 비율^2 + 양성클래스의 비율^2)      
- 두 개의 클래스가 완전히 섞여 있다면 불순도가 높음. 두 클래스의 비율이 정확히 1/2씩 이라면 지니불순도가 0.5가 되어 최악임(최댓값). 순수노드는 0임      
- 부모노드와 자식노드의 불순도 차이(**정보이득**)가 가능한 크도록, 즉 자식노드의 불순도를 최대한 낮아지게 하도록 트리를 성장시킴     
- **정보이득**(불순도 차이) = 부모의 불순도 - (왼쪽노드 샘플 수/부모의 샘플 수)x왼쪽노드 불순도 - (오른쪽노드 샘플 수/부모의 샘플 수)x오른쪽노드 불순도       
- 엔트로피 불순도 = -음성클래스의 비율xlog2(음성클래스의 비율) - 양성클래스의 비율xlog2(양성클래스의 비율)      
- 지니 불순도와 엔트로피 불순도가 만든 결과의 차이는 크지 않음      
- 이렇듯 불순도를 기준으로 샘플을 나누고, 불순도는 클래스별 비율을 가지고 계산한다. 따라서 특성값의 스케일은 계산에 영향을 미치지 않기 때문에, **결정트리는 표준화 전처리 과정이 필요없다**. -> 전처리 전의 훈련세트와 테스트세트로 모델을 훈련시키면 score점수가 동일하게 나온다.      

### 가지치기      
- 결정트리는 제한 없이 성장하면 훈련세트에 과대적합되기 쉽기 때문에 **가지치기**로 결정트리의 성장을 제한해주어야함      
- 가장 간단한 방법은 최대 깊이(max_depth)를 정하는 것      
```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
```
- **특성중요도**: 결정트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값      
- 각 노드의 정보이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더하여 계산      
- 이를 활용하여 결정트리모델을 특성 선택에 활용할 수 있음      
```python
print(dt.feature_importances_)
``` 


# 5-2 교차검증과 그리드 서치      
화이트와인과 레드와인의 **분류 성능**을 올리기 위해서는 **다양한 하이퍼파라미터**를 시도해보아야한다.       
하지만 이 과정에서 테스트세트를 사용하면 테스트세트에 맞춰 모델을 훈련하는 꼴이다.       
따라서 성능을 올바르게 예측하기 위해서는, 테스트세트는 최종모델을 선택할 때까지 사용하지 말아야 한다.       

### 검증세트      
테스트세트를 사용하지 않고 모델을 평가하기 위해 훈련세트에서 떼어낸 세트를 **검증세트**라고 한다.      
![image](https://github.com/user-attachments/assets/47135bd2-bad0-4018-86d0-34b6018b85bb)      
- 훈련세트에서 모델을 훈련하고, 검증세트로 모델을 평가 -> 매개변수를 바꿔가며 가장 좋은 모델 선택      
- 이 매개변수를 사용해 전체 훈련 데이터에서 모델을 다시 훈련 -> 테스트 세트에서 최종 점수 평가      
검증세트를 만들어보자      
```python
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)
sub_input, val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42)
print(sub_input.shape, val_input.shape)
# train_test_split() 함수를 2번 적용해서 훈련세트와 검증세트로 나누어줌

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))
```
### 교차검증      
하지만 검증세트가 크지 않다면 데이터를 나누는 기준에 따라 검증 점수가 들쭉날쭉할 것이다.       
따라서 모델성능을 안정적으로 평가하기 위해 검증세트를 여러번(k번) 나누어 모델을 여러 번 평가해야한다. 그를 **k-겹 교차검증**이라고 한다.      
훈련세트를 여러 폴드로 나눈 다음, 한 폴드씩 돌아가면서 검증세트의 역할을 하고 나머지 폴드로 모델을 훈련한다.        
**최종 점수**는 모든 폴드의 검증점수를 평균하여 계산한다.       
![image](https://github.com/user-attachments/assets/2fb0e8b3-a34c-48ff-a8f2-bd54217eb856)      
```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
```
![image](https://github.com/user-attachments/assets/db6c04f8-c1ec-4725-9eff-1b5d93108a2e)      
- 사이킷런 cross_validate() 교차검증함수에 평가할 모델 객체를 첫 번째 매개변수로 전달, 직접 검증세트를 떼어내지 않고 훈련세트 전체를 cross_validate()함수에 전달      
- fit_time, score_time, test_score 키를 가진 딕셔너리 반환 - test_score의 점수(검증폴드의 점수)를 평균한 것이 **교차검증의 최종점수** - print(np.mean(scores['test_score']))      
- 주의할 점) cross_validate()는 훈련세트를 섞어 폴드를 나누지 않기 때문에, 교차검증을 할 때 훈련세트를 섞으려면 **분할기**를 지정해야함 - cv      
- **분할기**는 교차검증에서 폴드를 어떻게 나눌지 결정해줌(데이터를 섞지 않고, 클래스 비율을 유지하며 폴드를 나눔, 만약 데이터를 섞고 싶다면 shuffle=True로 설정해야함) - 회귀: KFold 분할기, 분류: StratifiedKFold 사용      
```python
from sklearn.model_selection import StratifiedKFold
scores = cross_validate(dt, train_input, train_target, cv=StratifiedKFold())
print(np.mean(scores['test_score']))
```
만약, 훈련세트를 섞은 후 10-폴드 교차검증을 수행하려면,       
```python
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))
```

이어서 매개변수 값을 바꿔가면서 가장 좋은 성능이 나오는 모델을 찾아보자      
- 모델파라미터: 머신러닝모델이 학습하는 파라미터      
- 하이퍼파라미터: 모델이 학습할 수 없어서 (학습 시작 전에) 사용자가 지정해야만 하는 파라미터      
- 하이퍼파라미터 튜닝하는 법: 라이브러리가 제공하는 기본값 그대로 모델 훈련 - 최적의 하이퍼파라미터를 찾기위해, 검증세트의 점수나 교차검증을 통해서 매개변수를 바꿔줌- 매개변수를 바꿔가면서 모델을 훈련하고 교차검증 수행      
- 여러개의 매개변수를 튜닝할 때, 하나씩 하는 게 아니라 동시에 진행해주어야 함      

Q. 사람들은 하이퍼파라미터를 수동으로 설정하지말고, 그를 최적으로 지정해주는 autoML을 그냥 사용하면 되는거 아닌가?      
A. AutoML은 매우 유용한 도구이지만, 모든 경우에 이상적인 해결책은 아닙니다. 특히 복잡하거나 특화된 문제, 제한된 자원, 해석 가능성이 중요한 경우, 수동으로 하이퍼파라미터를 조정하는 것이 더 적합할 수 있습니다. AutoML은 반복적이고 표준화된 작업을 빠르게 처리하는 데 강력하지만, 전문 지식이 요구되는 상황에서는 수동 조정이 여전히 필요합니다.      

다양한 하이퍼파라미터를 탐색과 교차검증을 자동화하는 도구인 **그리드서치**를 이용하면 편하다.- cross_validate() 함수를 호출할 필요가 없다. 탐색할 매개변수를 나열하면, 교차검증을 수행하여 가장 좋은 검증 점수의 매개변수 조합을 선택한 후, 이 매개변수 조합으로 최종 모델을 훈련한다.       
```python
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]} # {탐색할 매개변수: 탐색할 값의 리스트}

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) # 객체 생성
# GridSearchCV의 cv 매개변수 기본값은 5임. min_impurity_decrease 값마다 5폴드교차검증을 수행하므로, 5x5=25개의 모델을 훈련함
# 많은 모델을 훈련하므로 n_jobs매개변수에서 병렬실행에 사용할 CPU코어 수를 지정해주어야함. -1로 지정하면 시스템에 있는 모든 코어를 사용함

gs.fit(train_input, train_target)
# 그리드 서치는 훈련이 끝나면 25개의 모델 중에서 검증점수가 가장 높은 모델의 매개변수 조합으로 전체 훈련세트에서 자동으로 다시 모델을 훈련함 - 이 모델은 gs객체의 best_estimator_에 저장되어있음
dt = gs.best_estimator_
print(dt.score(train_input, train_target)) #0.9615162593804117

# 최적의 매개변수(하이퍼파라미터)는 best_params_ 속성에 저장되어있음
print(gs.best_params_) #{'min_impurity_decrease': 0.0001}

# 각 매개변수에서 수행한 교차검증의 평균점수
print(gs.cv_results_['mean_test_score'])  #[0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]

# argmax()함수를 사용하여 교차검증 평균점수 중 가장 큰 값의 인덱스 추출, 그 다음 이 인덱스를 사용해 params키에 저장된 매개변수 출력
best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])  #{'min_impurity_decrease': 0.0001}
```
#### 정리하면 ... 1) 탐색할 매개변수 지정 2) 훈련세트에서 그리드서치를 수행해 최상의 평균 검증 점수가 나오는 매개변수 조합 찾기 3) 그리드서치는 최상의 매개변수에서 전체 훈련 세트를 사용해 최종 모델을 훈련함      

더 복잡한 매개변수 조합을 탐색해보자.       
```python
params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001), # 0.0001에서 시작하여 0.001까지 0.0001씩 증가
          'max_depth': range(5, 20, 1),
          'min_samples_split': range(2, 100, 10)
          }
# min_impurity_decrease은 9개(end값은 포함이 안되므로), max_depth 15개, min_samples_split 10개로 이 매개변수로 수행할 교차검증횟수는 9x15x10= 1350개이다. 기본 5-폴드교차검증을 수행하므로 만들어지는 모델의 수는 6750개이다.

#그리드서치 수행
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)

#최상의 매개변수 조합 확인
print(gs.best_params_) # {'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}
#최상의 교차검증 점수 확인
print(np.max(gs.cv_results_['mean_test_score'])) # 0.8683865773302731
```
      
**랜덤서치**: 매개변수 값이 수치형(연속적인 실숫값)이라면, 탐색할 값을 직접 나열하는 것이 아니라, 싸이파이의 확률분포객체를 전달하여 특정 범위 내에서 지정된 횟수만큼 매개변수 후보 값을 샘플링하여 교차 검증을 시도할 수 있다.        
이는 한정된 자원을 최대한 활용하여 효율적으로 하이퍼파라미터 공간을 탐색할 수 있는 아주 좋은 도구이다.        
- 확률분포객체: 특정 확률분포를 기반으로 무작위 값을 생성하는 도구, 난수 발생기와 유사       
- 랜덤 서치에서 이 객체를 사용해 매개변수 값을 다양하게 샘플링하고, 그 값들을 이용해 모델을 학습시켜 최적의 하이퍼파라미터를 찾음
예시)       
- 샘플링 횟수는 시스템 자원이 허락하는 범위 내에서 최대한 크게 하는 것이 좋음       
```python
from scipy.stats import uniform, randint #randint: 정숫값, uniform: 실숫값
# 0~10 사이의 범위를 갖는 randint 객체를 만들고 10번 샘플림
rgen = randint(0, 10)
rgen.rvs(10)
#array([4, 7, 6, 8, 9, 3, 8, 3, 1, 4])

# 1000번 샘플링 후, 각 숫자의 개수 세기 
np.unique(rgen.rvs(1000), return_counts=True)
#(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
 array([116, 105,  95, 100,  84,  90,  97,  95, 107, 111])) - 고르게 추출된 것을 확인할 수 있음

# 0~1 사이의 10개 실수 추출
ugen = uniform(0, 1)
ugen.rvs(10)
# array([0.07156624, 0.51330724, 0.78244744, 0.14237963, 0.05055468,
       0.13124955, 0.15801332, 0.99110938, 0.08459786, 0.92447632])
```
탐색할 매개변수의 딕셔너리를 만들어 보자        
```python
# 탐색할 매개변수 범위
params = {'min_impurity_decrease': uniform(0.0001, 0.001),
          'max_depth': randint(20, 50),
          'min_samples_split': randint(2, 25),
          'min_samples_leaf': randint(1, 25),
          }

# params에 정의된 매개변수 범위에서 총 100번(n_iter 매개변수)을 샘플링해서 교차검증을 수행하고, 최적의 매개변수 조합을 찾음
# 100번 샘플링한다는 것은, 전체 조합 중에서 임의로 100개의 조합을 골라서 탐색한다는 것. 조합의 수가 많아도 샘플링 횟수(n_iter)에 따라 검증 횟수를 제한할 수 있음 - 그리드 서치는 모든 조합을 시도함: 실제 최적의 조합이 될 확률이 낮은 부분에서도 많은 시간을 소
# 그리드 서치보다 훨씬 교차 검증 수를 줄이면서 넓은 영역을 효과적으로 탐색할 수 있음. 또한 훨씬 더 적은 시간과 자원 이용 
from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params,
                        n_iter=100, n_jobs=-1, random_state=42)
gs.fit(train_input, train_target)

# 최적의 매개변수 조합 출력
print(gs.best_params_)
#{'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13}

# 최고의 교차 검증 점수 확인인
print(np.max(gs.cv_results_['mean_test_score']))
# 0.8695428296438884

# 최적의 모델은 이미 전체 훈련세트로 훈련되어 best_estimator_속성에 저장되어있음. - 이 모델을 최종 모델로 결정하고, 테스트 세트의 성능을 확인해보자
dt = gs.best_estimator_
print(dt.score(test_input, test_target))
# 0.86
```       
# 5-3 트리의 앙상블        

### 정형 데이터와 비정형 데이터        
지금까지 배운 머신러닝 알고리즘은 정형 데이터에 잘 맞고, 그 중 정형데이터를 다루는데 가장 뛰어난 성과를 내는 알고리즘이 **앙상블 학습** : 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘.        

### 랜덤 포레스트        
가장 대표적인 결정 트리 기반의 앙상블 학습 방법.         
결정트리를 랜덤하게 만들어 결정트리의 숲을 만들고, 각 결정트리의 예측을 사용해 최종 예측을 만듦        
- 어떻게 숲을 구성하느냐?        
1) 각 트리를 훈련하기 위한 데이터를 랜덤하게 만듦: **부트스트랩 샘플 사용**        
  - 부트스트랩 샘플: 원래의 훈련데이터에서 중복을 허용하는 방식으로 랜덤하게 샘플을 추출해 **각 트리마다 고유한 훈련데이터**를 생성. 훈련세트의 크기와 동일        
  - ![image](https://github.com/user-attachments/assets/f9319bbf-3d7b-4d1e-a520-9f4d90361c23)        
2) 각 노드를 분할할 때 전체 특성 중에서 **일부 특성을 무작위로 고른** 다음 이 중에서 **최선의 분할**을 찾음        
  - 결정 트리는 각 노드에서 데이터를 분할하여, 데이터가 속하는 클래스나 예측 값을 보다 잘 구분할 수 있도록 함. 이때, 데이터를 어떻게 분할할지 결정하기 위해 트리는 특성을 활용        
  - 랜덤 포레스트는 여러 개의 결정 트리를 사용, 독립적으로 훈련시키는 각 트리들이 서로 비슷하지 않도록 하기 위해 무작위성을 도입 = 특성의 일부만 무작위로 선택 -> 트리 간의 다양성 증가 -> 앙상블의 성능을 높이는데 기여 & 모든 특성을 사용하면 특정 특성에 대해 과적합될 수 있지만, 일부만 사용함으로서 이러한 위험을 줄임.        
  - 최선의 분할을 찾다?! = 결정 트리에서 특정 노드에 있는 데이터를 어떻게 분할할지를 결정하는 과정에서, 가장 효과적으로 데이터를 구분할 수 있는 기준을 찾는다 - 불순도 이용. 자식노드의 순수도가 높아도록! -> 이 과정을 통해 결정 트리가 데이터를 더 잘 구분하게 되고, 예측의 정확도가 높아짐        
  - **분류**모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근 만큼 특성을 선택하고, **회귀**모델인 RandomForestRegressor는 전체 특성 사용        
  - ![image](https://github.com/user-attachments/assets/19c4cd89-8db4-44ad-b98d-6ac1969f6fb8)        
  - 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼고, 회귀일때는 각 트리의 예측을 평균함        
        
랜덤 포레스트는 랜덤하게 선택한 샘플1)과 특성2)을 사용하기 때문에 **훈련세트에 과대적합되는 것을 막고**, **검증세트와 테스트세트에서 안정적인 성능**을 얻을 수 있음        

이제 화이트와인 분류 문제에 적용해보자        . 
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
wine = pd.read_csv('https://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

# cross_validate() 함수를 사용해 교차 검증 수행. return_train_score=True로 지정하면 검증점수와 함께 훈련세트에 대한 점수도 같이 반환(과대적합 파악에 활용)
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9973541965122431 0.8905151032797809

# 특성중요도: 각 결정 트리의 특성 중요도를 취합한 것
rf.fit(train_input, train_target)
print(rf.feature_importances_)
# [0.23167441 0.50039841 0.26792718]
# 다른 특성들의 중요도가 골고루 올라감 -> 랜덤 포레스트가 특정 특성에 대한 의존도를 줄이고, 더 다양한 특성을 사용하여 예측을 수행했다는 것을 의미

# OOB 샘플(부트스트랩 샘플에 포함되지않고 남는 샘플)을 사용해 훈련한 결정 트리를 평가할 수 있음 = 검증세트의 역할
# 이 점수를 얻기 위해 oob_score=True로 지정하고 모델 훈련 후, OOB 점수 출력
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
# 0.8934000384837406
```

### 엑스트라 트리        
랜덤 포레스트와 비슷하게 결정트리를 사용하여 앙상블 모델을 만듬.         
기본적으로 100개의 결정 트리 훈련함        
**차이점:** 부트스트랩 샘플 사용 X(전체 훈련세트 사용).특성을 랜덤하게 선택하여 노드 분할하는데 사용함(최선X) -> 성능은 낮아져도, 많은 트리를 앙상블하기 때문에 과대적합을 막고 검증세트의 점수를 높임        
- 보통 엑스트라 트리의 무작위성이 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련함.        
- 하지만 랜덤하게 노드를 분할하기 때문에 계산 속도가 빠름        
```python
# 모델의 교차검증 점수 확인
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9974503966084433 0.8887848893166506

# 특성중요도
et.fit(train_input, train_target)
print(et.feature_importances_)
# [0.20183568 0.52242907 0.27573525]
```

### 그레이디언트 부스팅        
얕은 결정 트리(기본: 3)를 연속적으로 추가하여(기본: 100) 손실함수를 최소화함. 경사하강법을 사용하여 트리를 앙상블에 추가        
  - 분류; 로지스틱 손실 함수, 회귀; 평균제곱오차함수        
깊이가 얕은 결정트리를 사용하기 때문에 과대적합에 강하고, 높은 일반화 성능을 보임.         
  - 모델이 훈련 데이터에 과하게 학습하지 않기 때문에 과대적합 방지. 얕은 트리들은 단순한 규칙을 학습하기 때문에(복잡한 세부사항 학습 X), 노이즈에 민감하지 않고, 여러 개의 얕은 트리들이 결합되면서 더 강력한 예측 성능을 얻을 수 있음
성능이 뛰어나지만 병렬로 훈련할 수 없기 때문에, 훈련속도가 느림(n_jobs 매개변수가 없음)        
학습률 매개변수를 조정하여 모델 복잡도를 제어할 수 있음(학습률 매개변수가 크면 복잡하고 훈련세트에 과대적합된 모델을 얻을 수 있음). 학습률이 커지면 학습속도가 빨라짐짐        
  - 학습률: 각 단계에서 새로 추가되는 결정 트리가 이전 단계의 예측을 얼마나 수정할지를 결정하는 매개변수.        
```python
# 모델의 교차검증 점수 확인
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.8881086892152563 0.8720430147331015

# 성능 향상을 위해 학습률(learning_rate)을 증가시키고 트리의 개수(n_estimators)를 늘림
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9464595437171814 0.8780082549788999

# 특성중요도도
gb.fit(train_input, train_target)
print(gb.feature_importances_)
# [0.15872278 0.68010884 0.16116839]
````
+) 또 다른 매개변수 : subsample - 트리 훈련에 사용할 훈련세트의 비율을 정함        
- 기본값 1.0 : 전체 훈련세트 사용 / 그보다 작으면 훈련 세트의 일부를 사용 like 확률적 경사하강법, 미니배치경사하강법        

### 히스토그램 기반 그레이디언트 부스팅        
그레이디언트 부스팅의 속도와 성능을 더욱 개선한 것. 안정적인 결과와 높은 성능으로 매우 인기가 높음(가장 뛰어난 앙상블 학습으로 평가 받음).        
입력 특성을 256개 구간으로 나누어 사용하기 때문에 노드 분할 속도가 매우 빠름(특성의 세부적인 값을 모두 다루는 대신, 구간 단위로 처리하여 연산을 단순화할 수 있음)- 노드분할 시 최적의 분할을 매우 빠르게 찾음         
  - 그레이디언트 부스팅에서, 각 특성의 모든 값을 직접 사용하여 분할 기준을 찾음: 데이터가 많거나 특성이 복잡할 경우 계산 비용이 매우 높아짐        
  - 히스토그램 기반 방법: 이러한 계산 비용을 줄이기 위해, 연속형 특성을 일정한 수의 구간으로 분할하고, 각 구간에 속하는 데이터의 수(빈도)를 기록하여 히스토그램을 만듦. 이후, 이 히스토그램을 사용하여 분할 기준을 찾음        
  - 연속형 특성 ex) 키, 체중, 온도, 시간 - 무한히 많은 값을 가질 수 있고, 실수 값이며 자연적인 측정에서 얻어진 값임        
256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용함 - 입력에 누락된 특성이 있더라도, 이를 따로 전처리할 필요가 없음. 즉, 누락된 값이 있는 데이터를 모델에 그대로 입력해도 문제가 없음        
```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9321723946453317 0.8801241948619236

# 특성 중요도
from sklearn.inspection import permutation_importance
hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10,
                                random_state=42, n_jobs=-1)
print(result.importances_mean)
# [0.08876275 0.23438522 0.08027708]

result = permutation_importance(hgb, test_input, test_target, n_repeats=10,
                                random_state=42, n_jobs=-1)
print(result.importances_mean)
# [0.05969231 0.20238462 0.049     ]

# 테스트 세트에서의 성능
hgb.score(test_input, test_target)
# 0.8723076923076923
```
사이킷런 말고도 히스토그램 기반 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 여럿 있음 : 대표적으로 XGBoost, LightGBM        
```python
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

from lightgbm import LGBMClassifier
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
앙상블 학습과 그리드 서치, 랜덤서치를 사용한 하이퍼파라미터 튜닝을 사용하면 최고 수준의 성능을 내는 머신러닝 모델을 얻을 수 있음        

지금까지 배운 것은 입력과 타깃이 준비된 **지도학습**!!         


