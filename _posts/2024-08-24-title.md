---
layout: post
title:  "6. 비지도학습"
date : 2024-08-24 00:25:20 +0700 
---

# 6-1 군집 알고리즘       
고객들이 사고 싶은 과일 사진을 보내면, 그 중 가장 인기있는 과일을 판매하고자 한다.        
하지만 고객이 어떤 과일 사진을 보낼지는 알 수 없기 때문에, 미리 과일 분류기를 훈련할 수는 없다.        
이렇게 타깃이 없을 때는 비지도학습 알고리즘을 이용한다.        

비지도학습이란, 머신러닝의 한 종류로, 훈련데이터에 타깃이 없을 때 사용한다. 따라서, 외부의 도움 없이 스스로 유용한 무언가를 학습해야한다. ex. 군집, 차원축소       

## 과일 사진 데이터 준비하기       
먼저, 사과, 바나나, 파인애플의 흑백사진(npy파일로 저장)을 이용해 픽셀 값을 모두 평균내는 방식으로 분류해보자.       
``` python 
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
import numpy as np
import matplotlib.pyplot as plt

fruits = np.load('fruits_300.npy')
print(fruits.shape) #(300, 100, 100) #샘플의 개수, 행, 열
# 이미지의 크기가 100*100, 각 픽셀은 넘파이 배열의 원소 하나에 대응함

#첫 번째 이미지의 첫번째 행 출력
print(fruits[0, 0, :])
# [  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1
   2   2   2   2   2   2   1   1   1   1   1   1   1   1   2   3   2   1
   2   1   1   1   1   2   1   3   2   1   3   1   4   1   2   5   5   5
  19 148 192 117  28   1   1   2   1   4   1   1   3   1   1   1   1   1
   2   2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
   1   1   1   1   1   1   1   1   1   1]
# 흑백사진이므로 0~255의 정수값(밝기)을 가짐. 값이 높을수록 밝게 표시됨

plt.imshow(fruits[0], cmap='gray') #흑백이미지이므로 cmap='gray'지정해주기 - plt.imshow 함수가 이미지 데이터를 시각화할 때 자동으로 색상 맵(color map)을 적용하기 때문
plt.show()

fig, axs = plt.subplots(1, 2) # subplot() 함수로 여러 개의 그래프를 배열처럼 쌓을 수 있음, (1,2): 하나의 행, 2개의 열
axs[0].imshow(fruits[0], cmap='gray')
axs[1].imshow(fruits[0], cmap='gray_r')
plt.show()
```
- ![image](https://github.com/user-attachments/assets/5a5c417a-da48-48b8-b962-75214de9f0eb)       
- 보통 흑백사진은 바탕이 밝고 물체가 짙은색임. 하지만 첫번쨰 사진은 그와 반대로 되어있는 것을 확인할 수 있음       
- 이 흑백이미지는 이미지를 넘파이 배열로 변환할 때 반전시켰기 때문임 - 컴퓨터가 높은 값인 바탕에 집중하기에, 이를 방지하기 위함       
- 따라서 우리 눈에 보기 좋게 cmap의 매개변수를 'gray_r'로 지정해서 우리 눈에 보기 좋게 출력함 (이때는 밝은 부분이 0에 가까움)       

## 픽셀값 분석하기       
사용하기 쉽게 fruits 데이터를 사과, 파인애플, 바나나로 각각 나누어보자.       
배열 계산의 편리성을 위해 100x100 이미지를 펼쳐서 길이가 10000인 1차원 배열로 만들자.       
```python
apple = fruits[0:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)
# 이를 reshape(-1, 100*100)으로 변환-> 첫 번째 차원은 -1로 지정되어 자동으로 계산, 두 번째 차원은 10,000으로 고정
# 10,000개의 원소를 가질 2차원 배열에서 두 번째 차원이 10,000으로 지정되어 있으므로, 첫 번째 차원(-1)은 자동으로 100로 계산됨(샘플이 100개 이니까)

print(apple.shape)
# (100, 10000)

# 샘플의 픽셀 평균 계산 - axis=1로 지정해 열을 따라(가로 방향으로 이동하면서) 계산
print(apple.mean(axis=1))
# [ 88.3346  97.9249  87.3709  98.3703  92.8705  82.6439  94.4244  95.5999
  90.681   81.6226  87.0578  95.0745  93.8416  87.017   97.5078  87.2019
  88.9827 100.9158  92.7823 100.9184 104.9854  88.674   99.5643  97.2495
  94.1179  92.1935  95.1671  93.3322 102.8967  94.6695  90.5285  89.0744
  97.7641  97.2938 100.7564  90.5236 100.2542  85.8452  96.4615  97.1492
  90.711  102.3193  87.1629  89.8751  86.7327  86.3991  95.2865  89.1709
  96.8163  91.6604  96.1065  99.6829  94.9718  87.4812  89.2596  89.5268
  93.799   97.3983  87.151   97.825  103.22    94.4239  83.6657  83.5159
 102.8453  87.0379  91.2742 100.4848  93.8388  90.8568  97.4616  97.5022
  82.446   87.1789  96.9206  90.3135  90.565   97.6538  98.0919  93.6252
  87.3867  84.7073  89.1135  86.7646  88.7301  86.643   96.7323  97.2604
  81.9424  87.1687  97.2066  83.4712  95.9781  91.8096  98.4086 100.7823
 101.556  100.7027  91.6098  88.8976]

# 히스토그램(구간별로 값이 발생한 빈도를 그래프로 표시)을 통해 평균값 분포 확인
plt.hist(np.mean(apple, axis=1), alpha=0.8) # alpha로 투명도 조절
plt.hist(np.mean(pineapple, axis=1), alpha=0.8)
plt.hist(np.mean(banana, axis=1), alpha=0.8)
plt.legend(['apple', 'pineapple', 'banana'])
plt.show()
# 사과와 파인애플이 많이 겹쳐있어 픽셀값만으로는 구분하기 쉽지 않음 - 형태적, 크기적 유사성

# 픽셀별 평균값 계산
fig, axs = plt.subplots(1, 3, figsize=(20, 5)) # 1행 3열
axs[0].bar(range(10000), np.mean(apple, axis=0)) # 행을 따라 계산, 각 열마다 평균을 계산함
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()
# range(10000): 0부터 9999까지의 정수를 생성하여 x축의 위치를 나타내고, np.mean(apple, axis=0): 각 위치에서의 막대의 높이(y축 값)를 나타냄 - 막대그래프는 각 열의 평균을 시각화

# 픽셀 평균값을 100x100 크기로 바꿔서 이미지처럼 출력해서 비교
# 픽셀을 평균 낸 이미지 = 모든 사진을 합쳐놓은 대표 이미지
# apple.shape: (100, 10000) / np.mean(apple, axis=0).shape: (10000,)
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```
![image](https://github.com/user-attachments/assets/18bad03f-8a1c-4ad6-bd21-6f08c181adc9)     

픽셀별 평균값의 히스토그램        
![image](https://github.com/user-attachments/assets/11e8b362-e493-4037-8ec5-602f7996707b)    
Q. 파란색으로 겹쳐보이는 이유가 뭐지? 각 픽셀별 평균이면 하나의 값만 있는 것 아닌가?? 실제로 np.mean(apple, axis=0).shape의 출력값은 (10000,)인데?        

픽셀을 평균 낸 이미지        
![image](https://github.com/user-attachments/assets/4f3d7693-e5e0-43e1-852e-a013a5fa0329)        
이 대표 이미지와 가까운 사진을 골라내면 사과, 파인애플, 바나나를 구분할 수 있지 않을까?        

## 평균값과 가까운 사진 고르기        
사과 사진의 평균값인 apple_mean과 가장 가까운 사진을 고르기 위해 절대값 오차(모든 샘플에서 apple_mean 빼기)를 사용할 것이다.      
```python
# abs(): 절댓값 계산, 배열을 입력하면 입력과 동일한 크기의 배열 반환
abs_diff = np.abs(fruits - apple_mean) #abs_diff의 크기는 (300,100,100), apple_mean의 크기는 (100,100)
# fruits 배열의 각 이미지(100x100)에서 apple_mean을 뺀 후, 그 차이의 절대값을 계산
abs_mean = np.mean(abs_diff, axis=(1,2)) # 추가설명 참고
print(abs_mean.shape)
# (300,)
```
- 추가설명) abs_mean = np.mean(abs_diff, axis=(1,2))        
: axis) mean 함수가 어떤 축을 따라 평균을 계산할지를 지정        
-- axis=1: 100개의 행(첫 번째 100)에 대해 평균을 계산        
-- axis=2: 100개의 열(두 번째 100)에 대해 평균을 계산        
-- 결과: axis=(1,2)를 지정하면, 각 이미지에 대해 모든 픽셀(100x100)에 대한 평균 절대값 차이를 계산        
-- -> 따라서 각 이미지(100x100)에 대해 하나의 평균 절대 오차 값이 나옴. 따라서 fruits 배열의 300개의 샘플 각각에 대해 하나의 평균값이 계산되어, 결과적으로 (300,) 크기의 1차원 배열 생성        

```python
apple_index = np.argsort(abs_mean)[:100] #np.argsort: 배열을 오름차순으로 정렬 후 그 인덱스 반환-> apple_mean와 가까운 이미지들이 앞으로
# 이중 루프는 10x10 그리드의 각 위치에 이미지를 하나씩 채워 넣음
fig, axs = plt.subplots(10, 10, figsize=(10,10))
for i in range(10):          # 첫 번째 루프: 행을 순회
    for j in range(10):      # 두 번째 루프: 열을 순회
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
        axs[i, j].axis('off')
plt.show()
```
- 동작원리: i = 0일 때:        
첫 번째 행을 처리-> j = 0부터 9까지 반복하면서, 첫 번째 행의 10개의 열을 순회        
예를 들어, i=0이고 j=0일 때, axs[0, 0] 위치에 이미지를 그림. i=1이고 j=2라면 i*10 + j = 12가 되므로, apple_index[12]에 이미지를 그림          
![image](https://github.com/user-attachments/assets/2b3629a5-cc00-4b5e-af33-fff51271d9e3)        

- **군집**: 비슷한 샘플끼리 하나의 그룹으로 모으는 대표적인 비지도 학습 작업.
- 군집 알고리즘으로 모은 샘플그룹을 **클러스터**라고 함               


우리는 이미 사과, 파인애플, 바나나(타깃값)가 있는 것을 알고 있었기 때문에, 그 평균값을 계산해서 가장 가까운 과일 값을 찾을 수 있었음      
하지만 실제 비지도 학습에서는 타깃값을 모르기 때문에 이처럼 샘플의 평균값을 미리 구할 수 없음        
타깃값을 모르면서 어떻게 세 과일의 평균값을 찾을 수 있을까?        

# 6-2 k- 평균        
사진에 어떤 과일이 들어있는지 모를 때, 어떻게 평균값을 구할 수 있을까?        
**k- 평균 군집 알고리즘**이 **평균값**을 자동으로 찾아준다. 이 평균값이 클러스터의 중심에 위치하기 때문에 **클러스터 중심(센트로이드)**이라고 부름 - k- 평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값        

## k- 평균 알고리즘 소개         
k- 평균 알고리즘은 처음에 랜덤하게 k개의 클러스터 중심을 정하고, 그에 가장 가까운 샘플들로 클러스터를 만듦. 그 다음 클러스터의 중심을 다시 계산해서 이동하고 다시 클러스터를 만드는 식으로 반복해서 최적의 클러스터를 구성하는 알고리즘         
![image](https://github.com/user-attachments/assets/acb9dc1f-a76c-4e77-b9b3-99fb371cb09d)        

## KMeans 클래스        
```python
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
import numpy as np
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100) #3차원 배열을 2차원 배열로 변경

# k-평균 알고리즘 
from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)

# 군집된 결과(각 이미지에 할당된 클러스터의 레이블(번호))는 labels_ 속성에 저장됨
# labels의 길이는 샘플의 개수와 동일. n_clusters=3으로 설정했기 때문에, labels_ 배열의 각 값은 0, 1, 2 중 하나
# 클러스터 순서(번호)는 특정한 의미가 있는 것이 아니라 단순히 구분을 위한 식별자임
print(km.labels_)
# [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1]

# 레이블 0,1,2로 모은 샘플의 개수 확인
print(np.unique(km.labels_, return_counts=True))
# (array([0, 1, 2], dtype=int32), array([111,  98,  91]))

# 각 클러스터링된 이미지를 시각적으로 출력하기 위한 함수
import matplotlib.pyplot as plt
def draw_fruits(arr, ratio=1):      # arr: 시각화할 이미지 배열, ratio: 각 이미지의 크기를 조절하는 비율
    n = len(arr)    # n= 샘플 개수

    # 한 줄에 최대 10개의 이미지를 배치하기 위해, 샘플 개수를 10으로 나누어 전체 행 개수를 계산
    # np. ceil()함수는 입력된 숫자를 반올림하여 가장 가까운 정수로 만들어줌 
    rows = int(np.ceil(n/10)) # 아래의 예시 참고! 

    # 행이 1개면(rows < 2) 열 개수는 샘플 개수. 그렇지 않으면 10개
    cols = n if rows < 2 else 10
    # 서브플롯 생성
    fig, axs = plt.subplots(rows, cols,
                            figsize=(cols*ratio, rows*ratio), squeeze=False)
    # 이중 루프를 통한 이미지 배치
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:    # n 개까지만 그립니다.
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()
```
예시) 샘플 개수 n이 25인 경우:        
rows = ceil(25/10) = 3        
cols = 10 (since rows >= 2)        
그리드는 3행 10열로 구성되며, 총 30개의 서브플롯이 생성, 실제로는 25개의 이미지만 있으므로, 마지막 5개의 서브플롯은 빈 상태로        

이제 이 함수를 사용해 레이블이 0인 과일 사진을 그려보자.         
넘파이는 불리언 배열을 사용해 원소를 선택할 수 있다. km.lebels_ ==0  불리언 인덱싱        
![image](https://github.com/user-attachments/assets/b1ace414-24da-4465-8a17-6a4bbc756316)        
샘플들을 완벽하게 구별하지는 못하지만, 훈련데이터에 타깃 레이블을 제공하지 않은 것을 고려하면 훌륭한 결과이다.         

## 클러스터 중심        
클러스터 중심은 cluster_centers_ 속성에 저장되어있음        
이 배열은 fruits_2d 샘플의 클러스터 중심이기 때문에 이미지로 출력하기 위해 2차원 배열로 바꾸어주어야 함        
```python
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```
![image](https://github.com/user-attachments/assets/eaa54d09-8ef1-4c4f-a48a-b9aba6d70a53)        

```python
# 훈련데이터 샘플에서 클러스터 중심까지 거리로 변환
print(km.transform(fruits_2d[100:101]))
# [[3393.8136117  8837.37750892 5267.70439881]]
```
- KMeans의 transform 메서드는 입력 배열의 형태에 민감하게 작동: 2차원 배열을 입력으로 받기를 기대함         
- fruits_2d[100]: fruits_2d 배열(2차원(300,10000))에서 100번째 샘플(하나의 행)을 선택(인덱싱)함-> 이때 반환되는 것은 1차원 배열, 그 형태는 (10000,)로 단일 데이터 포인트(벡터)임        
- fruits_2d[100:101]: fruits_2d 배열에서 100번째 샘플만을 포함하는 부분배열을 선택(슬라이싱)함 -> 이 경우 반환되는 것은 2차원 배열, 그 형태는 (1, 10000)임. 이는 하나의 샘플을 포함하는 2차원 배열인 것         

```python
print(km.predict(fruits_2d[100:101]))
# [0] : 가장 가까운 클러스터 중심을 예측 클래스로 출력

# 샘플 그려보기
draw_fruits(fruits[100:101])

# 최적의 클러스터를 찾기 위해 알고리즘이 반복한 횟수
print(km.n_iter_)
# 4
```
클러스터 중심을 특성 공학처럼 사용해 데이터셋을 저차원으로 변환할 수 있다.         
- 특성 공학처럼 사용: 클러스터 중심을 활용해 데이터를 변환하는 과정, 원래 고차원 데이터를 클러스터 중심까지의 거리 등으로 변환하여 저차원 데이터로 만드는 것을 의미함        
- 각 데이터 포인트에서 각 클러스터 중심까지의 거리를 계산하여, 이 거리를 새로운 특성으로 사용함-> 원래 10000차원이었던 데이터가 클러스터 중심의 개수(여기서는 3개)만큼의 차원으로 줄어듭니다.        
- 예를 들어, 각 데이터 포인트가 (3393.81, 8837.37, 5267.70)이라는 세 개의 거리를 가지게 됨. 이는 원래의 10000차원 공간에서 3차원으로 변환된 것        
- 차원 축소는 데이터 분석, 시각화 또는 모델의 성능 향상에 유용할 수 있습니다.        

우리는 타깃이 3개인 것을 알고 있었기 때문에 n_clusters를 3으로 지정할 수 있었다.         
하지만 실전에서는 클러스터의 개수를 알 수 없다. 하지만 k-평균 알고리즘은 클러스터 개수를 사전에 지정해주어야 한다.         
이런 경우 최적의 클러스터 개수를 어떻게 구할 수 있을까?        

## 최적의 k찾기         
**엘보우 방법**: 최적의 클러스터 개수를 정하는 방법 중 하나.         
**이너셔**: 클러스터 중심과 샘플 사이 거리의 제곱합. 클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지를 나타내는 값(값이 작으면 클러스터 중심 주위에 가깝게 모여 있음을 나타냄)        
일반적으로 클러스터 개수가 늘어나면 클러스터 개개의 크기는 줄어들기 때문에 이니셔도 줄어듬        
**엘보우 방법**은 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법임 - 클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 클러스터 개수 k임                
![image](https://github.com/user-attachments/assets/4090d59d-3a43-4953-9e44-408ee53cc701)        

``` python
inertia = []
for k in range(2, 7):
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)

plt.plot(range(2, 7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```
![image](https://github.com/user-attachments/assets/6c679b58-11bc-4168-a599-013466c7e2d0)        
뚜렷하지는 않지만 k=3에서 그래프의 기울기가 조금 바뀐 것을 볼 수 있음        























