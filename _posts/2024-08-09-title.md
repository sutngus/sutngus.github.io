---
layout: post
title:  "로지스틱 회귀"
date : 2024-08-09 00:25:20 +0700
---
# 4-1 로지스틱 회귀     
생선 럭키백을 만든다고 가정해보자.      
럭키백에 무엇이 들어있는지는 알 수 없고, 포함된 생선의 확률만 알 수 있다.      
생선의 확률을 어떻게 구할 수 있을까?     

### 럭키백의 확률
k-최근접 이웃 분류기를 이용해서 이웃 클래스 비율을 확률이라고 출력할 수 있다.     
![image](https://github.com/user-attachments/assets/7e64b8df-df30-443c-bb94-530bd6ddc9d1)     
![image](https://github.com/user-attachments/assets/3d6e6d49-e902-4d5d-8450-9f73dca0d5ee)     
- read_csv()함수로 csv 파일을 데이터 프레인으로 변환 (csv파일의 첫줄을 자동으로 열제목으로 만들어줌)     
- 판다스의 unique 함수를 통해 어떤 종류의 생선이 있는지 species열에서 고유한 값 추출     
- species 열을 타깃으로 만들고 나머지 5개 열은 입력데이터로 사용 - 대괄호 두 개 [[ ]]는 여러 열을 선택할 때 사용     
![image](https://github.com/user-attachments/assets/d4f63097-2748-416c-8503-20e49d6dfa48)     
- 데이터를 훈련세트와 테스트세트로 나눔     
- 훈련세트와 테스트세트를 표준화 전처리 해줌 - 훈련세트의 통계값으로 테스트세트를 변환해야함     

k-최근접 이웃 분류기로 확률을 예측해보자     
- 타깃 데이터에 2개 이상의 클래스가 포함되어 있는 문제를 **다중 분류**라고 함     
![image](https://github.com/user-attachments/assets/f342cdab-a261-4dbf-82f7-7543dbdd6326)     
- 훈련세트와 테스트 점수는 현재 필요 X     
![image](https://github.com/user-attachments/assets/fb69694e-4b38-45ca-a0ca-03f13b79051f)     
- classes_속성에 KNeighborsClassifier에서 정렬된 타깃값 저장되어있음 - 이름순     
- predict()메서드로 타깃 값에 대한 예측 출력     
- predict_proba()메서드로 클래스별 확률 값 반환     
- 네 번째 샘플의 최근접 이웃 클래스의 indexes를 확인함으로서 계산한 확률이 가장 가까운 이웃의 비율이 맞는지 확인 가능     
한계: 3개의 최근접 이웃을 사용하기 때문에 가능한 확률은 : 0/3, 1/3, 2/3, 3/3이 전부임. 항상 정해진 확률만 출력함     

### 로지스틱 회귀     
- 로지스틱 회귀는 이름은 회귀이지만 **분류**모델이다.     
- 선형회귀와 동일하게 선형방정식을 학습함    
- ![image](https://github.com/user-attachments/assets/617f93d7-0f24-436e-8033-391fcd542640)     
- z는 어떤 값도 가능하지만, 확률이 되려면 0~1 사이의 값이 되어야하므로 시그모이드 함수를 사용함     
-  ![image](https://github.com/user-attachments/assets/75f74c74-a6d0-496d-846f-6d7fc26ff47c)     
-  ![image](https://github.com/user-attachments/assets/62b8f5aa-0e66-4504-ac09-b664d00859e4)

+)     
훈련 전 간단한 **이진 분류**(도미, 빙어)를 해보자 - 시그모이드 함수 출력이 0.5보다 크면 양성 클래스, 작으면 음성 클래스로 판단     
    - 넘파이 배열은 True, False 값을 전달해서 행을 선택함 : **불리언 인덱싱**     
    - ![image](https://github.com/user-attachments/assets/206c589a-143d-46d4-ae3f-aeecee588081)     
    - 불리언 인덱싱은 True에 해당하는 위치의 값을 선택, False 값은 무시-> True에 해당하는 인덱스 0과 2의 값인 'A'와 'C'가 선택됨     

위와 같은 방식으로 훈련세트에서 도미(Brem)와 빙어(Smelt)의 행만 골라내보자     
    - ![image](https://github.com/user-attachments/assets/ecb55756-3497-425f-b6d1-d1bb268a7f7b)     
    - 비교 연산자를 사용하면 도미와 빙어의 행을 모두 True로 만들고 골라낼 수 있음 - ex) train_target == 'Bream'     
    - OR 연산자 (|)를 통해서 도미와 빙어에 대한 행을 골라낼 수 있음     
    - 이 데이터로 로지스틱 회귀 모델을 훈련하면     
    - ![image](https://github.com/user-attachments/assets/30068c95-1cce-46fb-bd7f-8c4810bbfdb5)     
    - ![image](https://github.com/user-attachments/assets/d6d4e5c2-f7f3-496d-9979-33143444454c)     
    - 훈련한 모델을 사용해 train_bream_smelt의 첫 5개 샘플을 예측함     
    - **predict_proba()** 메서드로 예측한 음성클래스와 양성클래스에 대한 확률 출력     
    - classes_ 속성을 이용해 어떤 것이 음성(0)이고, 양성(1)인지 확인 - 빙어(S)가 양성     
    - lr.coef_, lr.intercept_로 로지스틱 회귀가 학습한 선형 방정식의 계수 확인     
    - 따라서 로지스틱 회귀가 학습한 방정식은 ![image](https://github.com/user-attachments/assets/3a1269b2-9d74-419c-95c1-f6621ccd36d6)인 것을 알 수 있다.     
    - **decision_function()** 메서드로 양성 클래스에 대한 z값 출력     
    - np.exp()함수를 사용해 z값을 시그모이드 함수에 통과시켜 확률을 얻을 수 있음 => predict_proba() 메서드 출력의 두번째 열의 값과 동일함  = decision_function() 메서드는 양성클래스에 대한 z값을 반환한다는 것을 알 수 있다.      
    
    Q. 이진분류에서 predict_proba 출력 결과의 2번째 열의 값과 decision_function 메서드 + 시그모이드 함수 결과는 동일하다. 그렇다면 predict_proba을 사용해서 그 2번째 열의 값을 보면 되지 왜 추가적으로 decision_function 메서드 + 시그모이드 함수를 추가적으로 사용하는거지??


이제 로지스틱 회귀로 다중분류를 수행해보자     
![image](https://github.com/user-attachments/assets/2ca1b690-a1b3-40fa-bfb6-61677c7def98)     
- LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용함 - max_iter 매개변수에서 반복횟수를 지정함     
- LogisticRegression은 기본적으로 계수의 제곱을 규제함(like 릿지) : L2 규제 - 규제 제어 매개변수: C(작을수록 규제가 커짐)     
- 여기서는 7개의 생선데이터가 모두 들어있는 train_scaled, train_target을 사용함     
![image](https://github.com/user-attachments/assets/123e991a-87ed-463d-ba25-8f0f5e312909)     
- 테스트세트 첫 5개 샘플에 대한 예측과 예측확률 출력     
![image](https://github.com/user-attachments/assets/981b9644-d768-4ba8-b89b-363fe766f94a)     
- 다중 분류의 선형방정식의 coef_, intercept_의 크기를 출력     
- 이 데이터는 5개의 특성을 사용하므로 coef_의 배열의 열은 5개임     
- coef_의 배열의 행과 intercept_이 7임: z를 7개 계산한다는 의미 - 다중분류는 클래스마다 z값을 하나씩 계산하고 가장 높은 z값을 출력하는 클래스가 예측 클래스가 됨     
- 다중분류는 소프트맥스 함수를 사용해 7개의 z값을 확률로 변환 (이진분류는 시그모이드함수 이용)     
- 소프트맥스 함수는 다중분류에서 여러 선형방정식의 출력 결과를 정규화하여 합이 1이 되도록 만듬     
- 소프트맥스 ![image](https://github.com/user-attachments/assets/917393b0-2b75-4564-8056-07896d1c687d)     
![image](https://github.com/user-attachments/assets/b3c13370-090e-4b49-85ae-d1ab68b0b78c)          
- decision_function()메서드로 z1_z7까지의 값을 구한 다음 소프트맥스 함수를 사용해 확률로 변환     
- softmax()의 axis 매개변수는 소프트맥스를 계산할 축을 지정함. - axis=1으로 지정해 각 행, 즉 각 샘플에 대해 소프트맥스 계산 / axis 매개변수를 지정하지 않으면 배열 전체에 대한 소프트맥스를 계산함     
- 앞서 구한 proba 배열과 결과가 일치하므로 성공 !     

    Q. coef_의 배열의 행과 intercept_이 7이다. 이는 클래스 수가 7개여서 각 클래스에 대해 하나씩, 촏 7개의 선형방정식이 만들어진다. 그렇다면 클래스가 2개인 이진분류에서는 2개의 선형방정식이 필요하지 않고, 하나만 만들어지는가?
      이진 분류에서는 클래스가 2개밖에 없으므로, 두 클래스 중 하나를 기준으로 확률을 계산합니다.
      하나의 선형 방정식을 사용해서 z값을 계산하고, 이 z값을 시그모이드 함수에 넣어 클래스 1일 확률을 구합니다. 이 확률이 0.5 이상이면 클래스 1로, 그 이하면 클래스 0으로 분류합니다.
      클래스가 2개이므로, 하나의 선형 방정식만으로도 두 클래스 중 어떤 클래스에 속할지 충분히 결정할 수 있습니다. - 클래스 1에 대한 z값을 계산하면, 클래스 0에 대한 z값은 자동으로 정해집니다(1에서 빼면 됨).



# 4-2 확률적 경사 하강법     
생선을 실시간으로 학습하기 위한 머신러닝 모델이 필요 -> **확률적 경사 하강법**을 사용해 **점진적**으로 학습하는 로지스틱 회귀 모델 훈련     
- **확률적 경사하강법**: 훈련세트에서 샘플 하나씩 랜덤하게 꺼내 손실 함수의 경사(기울기)를 따라 손실을 줄이면서 원하는 지점에 도달하는, 최적의 모델을 찾는 알고리즘     
- 손실함수: 확률적 경사하강법이 최적화할 대상. 대부분의 문제에 잘 맞는 손실 함수가 이미 정의되어있음 - 이진분류: 로지스틱회귀(이진크로스엔트로피) 손실함수/ 다중분류: 크로스엔트로피 손실함수/ 회귀문제: 평균제곱근 오차     
- 에포크: 확률적 경사 하강법에서 훈련세트 전체 샘플을 모두 사용하는 한 번의 반복. 훈련 데이터셋에 있는 모든 샘플을 한 번씩 모델에 입력하고, 그에 따른 손실을 계산하고, 이 손실을 줄이기 위해 모델의 **가중치를 업데이트**하는 과정을 한 번 반복하는 것. 일반적으로 수십~수백 번의 에포크 반복     
- 미니배치 경사하강법: 한 번 경사로를 따라 이동하기 위해 1개가 아닌 무작위로 몇 개의 샘플을 선택     
- 배치 경사하강법: 한 번 경사로를 따라 이동하기 위해 전체 샘플 사용. 가장 안정적일 수 있지만 컴퓨터 자원을 많이 사용하고, 데이터가 너무 많은 경우 한 번에 전체 데이터를 모두 읽을 수 없음     
![image](https://github.com/user-attachments/assets/ce3b1170-a9f8-4b55-9658-84a905b6d04c)     

### 손실함수     
- **분류**     
  - '손실 = 오답' 손실함수로 그냥 정확도로 설정하면, 연속적이지 않기 때문에 적당하지 않음(미분 불가능)     
  -  ![image](https://github.com/user-attachments/assets/697576e8-4951-4cf2-ac85-85dabfb200a7) 이 4개의 샘플의 예측확률을 0.9, 0.3, 0.2, 0.8 이라고 가정해보자     
  -  * 예측확률) 각각의 샘플이 특정 클래스(예: 클래스 1)일 확률 => 이를 통해 연속적인 손실함수를 얻을 수 있음     
  - ![image](https://github.com/user-attachments/assets/75391ed7-ed3e-41d3-9152-3b9ad3d123d2)     
  - 예측이 1에 가까울수록 예측과 타깃의 곱의 음수는 점점 더 작아짐 (-0.9보다 -0.3이 더 높은 손실)     
  - 타깃이 0일때는 1로 바꾸어주고, 예측값도 양성 클래스에 대한 예측으로 바꾸어주어야함     
  - 예측확률에 로그함수를 적용하면 더 좋음. 예측확률의 범위가 0~1 사이이기에 로그함수에서는 음수이고, 최종 손실 값은 양수가 됨 - 확률이 1에 멀어질수록 손실이 아주 큰 양수가 됨     
"이렇게 정의한 손실 함수를 로지스틱 손실함수, 또는 이진 크로스엔트로피 손실함수라고 한다." -> 이 손실 함수를 사용하면 로지스틱 회귀모델이 만들어짐 (다중분류: 크로스엔트로피 손실함수/ 회귀: 평슌제곱오차)     

즉, 산을 내려가면서 손실을 최소화하는, 즉 가장 낮은 지점에 도달해야함.      
이 산의 높이는 로지스틱 손실 함수(모델의 예측과 실제 값의 차이)로 측정됨      
로지스틱 손실 함수는 경사를 계산하는 데 중요한 역할을 함.      
손실 함수의 경사(기울기)를 계산하고, 그 정보를 사용하여 어느 방향으로 얼마나 이동할지를 결정     
(손실함수가 크면 빠르게,손실함수가 작아지면 작은 걸음으로 최적점을 찾아감)     

```python
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
#표준화 - 훈련세트 기준으로 테스트세트 변환
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

```python
from sklearn.linear_model import SGDClassifier #분류용
sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
#SGDClassifier의 2개의 매개변수: loss: 손실함수 종류 지정, max_iter: 수행할 에포크 횟수 지정
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
# 적은 에포크수로 인해 낮은 정확도 출력

# partial_fit을 이용해서 훈련한 모델 sc를 추가적으로 훈련 가능 
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```
### 에포크와 과대/ 과소적합      
에포크가 증가할수록 정확도가 향상된다. 하지만 얼마나 더 훈련하는게 최적의 결과를 도출할 수 있을까?      
에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다.       
- 에포크 횟수가 적으면 모델이 훈련세트를 덜 학습함 - 과소적합      
- 에포크 횟수가 많으면 훈련세트에 아주 잘 맞는 모델이 만들어짐 - 과대적합      

- 조기종료: 과대적합이 시작하기 전에 훈련을 멈추는 것      

```python
import numpy as np
sc = SGDClassifier(loss='log_loss', random_state=42)
train_score = []
test_score = []
classes = np.unique(train_target)
# 에포크마다 훈련세트와 테스트세트에 대한 점수를 기록하기 위해 2개의 리스트 준비

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
#300번의 에포크 동안 훈련 반복실행. 반복마다 점수를 계산하여 train_score, test_score 리스트에 추가

import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```
![image](https://github.com/user-attachments/assets/3d04c82f-88cf-4ed0-aded-efc1eeaa1ebf)      
100번째 에포크 이후 훈련세트와 테스트세트의 점수가 벌어짐.      
100번이 적절해보이므로 반복횟수를 100에 맞추고 모델을 훈련해보자      

```python
sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

sc = SGDClassifier(loss='hinge', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
# loss의 매개변수를 힌지손실(서포트벡터머신의 손실함수)로 설정
# 여러 종류의 손실 함수를 loss 매개변수에 지정할 수 있음!
```

